{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aadee827",
   "metadata": {},
   "source": [
    "# Constructing The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eae4d5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b726fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import IPython.display as ipd\n",
    "import keras\n",
    "import librosa # Compatible with python 3.10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Conv2D, Dense, Dropout, Flatten, MaxPool2D, Reshape\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae8e4f",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e563732f",
   "metadata": {},
   "source": [
    "#### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a6e5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign paths\n",
    "base_dir = \"dataset\"\n",
    "\n",
    "# Use for-norm dataset\n",
    "train_dir = os.path.join(base_dir, \"for-norm\", \"for-norm\", \"training\")\n",
    "test_dir = os.path.join(base_dir, \"for-norm\", \"for-norm\", \"testing\")\n",
    "val_dir = os.path.join(base_dir, \"for-norm\", \"for-norm\", \"validation\")\n",
    "\n",
    "# Classified directories\n",
    "train_dir_fake = os.path.join(train_dir, \"fake\")\n",
    "train_dir_real = os.path.join(train_dir, \"real\")\n",
    "test_dir_fake = os.path.join(test_dir, \"fake\")\n",
    "test_dir_real = os.path.join(test_dir, \"real\")\n",
    "val_dir_fake = os.path.join(val_dir, \"fake\")\n",
    "val_dir_real = os.path.join(val_dir, \"real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a108b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: dataset\n",
      "\n",
      "Train directory: dataset/for-norm/for-norm/training\n",
      "Test directory: dataset/for-norm/for-norm/testing\n",
      "Val directory: dataset/for-norm/for-norm/validation\n",
      "\n",
      "Train (fake) directory: dataset/for-norm/for-norm/training/fake\n",
      "Train (real) directory: dataset/for-norm/for-norm/training/real\n",
      "Test (fake) directory: dataset/for-norm/for-norm/testing/fake\n",
      "Test (real) directory: dataset/for-norm/for-norm/testing/real\n",
      "Val (fake) directory: dataset/for-norm/for-norm/validation/fake\n",
      "Val (real) directory: dataset/for-norm/for-norm/validation/real\n"
     ]
    }
   ],
   "source": [
    "# Check for mistakes\n",
    "print(\"Base directory:\", base_dir)\n",
    "print()\n",
    "print(\"Train directory:\", train_dir)\n",
    "print(\"Test directory:\", test_dir)\n",
    "print(\"Val directory:\", val_dir)\n",
    "print()\n",
    "print(\"Train (fake) directory:\", train_dir_fake)\n",
    "print(\"Train (real) directory:\", train_dir_real)\n",
    "print(\"Test (fake) directory:\", test_dir_fake)\n",
    "print(\"Test (real) directory:\", test_dir_real)\n",
    "print(\"Val (fake) directory:\", val_dir_fake)\n",
    "print(\"Val (real) directory:\", val_dir_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933b871e",
   "metadata": {},
   "source": [
    "#### Other Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43d1e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 12\n",
    "EPOCHS = 20\n",
    "STEPS_PER_EPOCH = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be882e7",
   "metadata": {},
   "source": [
    "## Obtain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4cc98ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain waveform (.wav) audio files\n",
    "train_fake_audio_path = [os.path.join(train_dir_fake, file) for file in os.listdir(train_dir_fake) if file.endswith(\".wav\")]\n",
    "train_real_audio_path = [os.path.join(train_dir_real, file) for file in os.listdir(train_dir_real) if file.endswith(\".wav\")]\n",
    "\n",
    "test_fake_audio_path = [os.path.join(test_dir_fake, file) for file in os.listdir(test_dir_fake) if file.endswith(\".wav\")]\n",
    "test_real_audio_path = [os.path.join(test_dir_real, file) for file in os.listdir(test_dir_real) if file.endswith(\".wav\")]\n",
    "\n",
    "validation_fake_audio_path = [os.path.join(val_dir_fake, file) for file in os.listdir(val_dir_fake) if file.endswith(\".wav\")]\n",
    "validation_real_audio_path = [os.path.join(val_dir_real, file) for file in os.listdir(val_dir_real) if file.endswith(\".wav\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51fdd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert audio file to spectrogram\n",
    "# def create_spectrogram(file_path):\n",
    "#     audio_data, sample_rate = librosa.load(file_path)\n",
    "#     spectrogram = librosa.stft(audio_data)\n",
    "#     decibel_spectrogram = librosa.amplitude_to_db(abs(spectrogram))\n",
    "#     return decibel_spectrogram\n",
    "\n",
    "# Convert audio file to mel-scale spectrogram\n",
    "# See https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53\n",
    "def create_mel_spectrogram(file_path):\n",
    "    audio_data, sample_rate = librosa.load(file_path)  \n",
    "    # Convert audio to mel-based spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
    "    # Convert from amplitude squared to decibel units\n",
    "    mel_decibel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)  \n",
    "    return mel_decibel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e39741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spectrograms as features to train the model\n",
    "def get_features_and_labels(real_audio_files, fake_audio_files):\n",
    "    spec_arr = []\n",
    "    labels = []\n",
    "\n",
    "    target_shape = (128, 87) # Target shape for resizing\n",
    "\n",
    "    for file in real_audio_files:\n",
    "        spectrogram = create_mel_spectrogram(file)\n",
    "        resized_spectrogram = cv2.resize(spectrogram, target_shape[::-1], interpolation=cv2.INTER_LINEAR) # Resize, swap width and height\n",
    "        spec_arr.append(resized_spectrogram)\n",
    "        labels.append(0)\n",
    "    for file in fake_audio_files:\n",
    "        spectrogram = create_mel_spectrogram(file)\n",
    "        resized_spectrogram = cv2.resize(spectrogram, target_shape[::-1], interpolation=cv2.INTER_LINEAR) # Resize, swap width and height\n",
    "        spec_arr.append(resized_spectrogram)\n",
    "        labels.append(1)\n",
    "\n",
    "\n",
    "    return np.array(spec_arr), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aed624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/pooh555/Anaconda/miniconda3/audio_classification/lib/python3.10/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = get_features_and_labels(train_real_audio_path, train_fake_audio_path)\n",
    "validation_features, validation_labels = get_features_and_labels(validation_real_audio_path, validation_fake_audio_path)\n",
    "test_features, test_labels = get_features_and_labels(test_real_audio_path, test_fake_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8a22cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verity the shapes of the features\n",
    "print(\"train features shape: {}\".format(train_features.shape))\n",
    "print(\"test features shape: {}\".format(test_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef311982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Significantly trimmed VGG model to optimize results\n",
    "\n",
    "trimmed_vgg = Sequential()\n",
    "trimmed_vgg.add(Reshape((128, 87, 1),input_shape=train_features.shape[1:]))\n",
    "\n",
    "trimmed_vgg.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation='relu'))\n",
    "trimmed_vgg.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation='relu'))\n",
    "trimmed_vgg.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "trimmed_vgg.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation='relu'))\n",
    "trimmed_vgg.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation='relu'))\n",
    "trimmed_vgg.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "trimmed_vgg.add(Flatten())\n",
    "trimmed_vgg.add(Dense(units=256,activation=\"relu\"))\n",
    "trimmed_vgg.add(Dense(units=256,activation=\"relu\"))\n",
    "trimmed_vgg.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "trimmed_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48a6fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "# Adam optimiser helps model get unstuck when stuck at local minima\n",
    "trimmed_vgg.compile(optimizer=keras.optimizers.Adam(),\n",
    "                    loss=keras.losses.binary_crossentropy, \n",
    "                    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ffa967",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_vgg_history = trimmed_vgg.fit(train_features,\n",
    "                                      train_labels,\n",
    "                                      validation_data = [validation_features, validation_labels],\n",
    "                                      batch_size = BATCH_SIZE,\n",
    "                                      epochs = EPOCHS,\n",
    "                                      steps_per_epoch = STEPS_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6091701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that extremely high accuracy is not due to model overfitting,\n",
    "# it's because the dataset is very \"easy\" relative to actual modern state of deepfaked audio\n",
    "# i.e. the fakes are mostly easily detectable, even to the human ear\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "ax1.plot(trimmed_vgg_history.history[\"accuracy\"])\n",
    "ax1.plot(trimmed_vgg_history.history['val_accuracy'])\n",
    "ax1.set_title(\"Accuracy\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.legend([\"Accuracy\",\"Validation Accuracy\"])\n",
    "\n",
    "ax2.plot(trimmed_vgg_history.history[\"loss\"])\n",
    "ax2.plot(trimmed_vgg_history.history[\"val_loss\"])\n",
    "ax2.set_title(\"Loss\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.legend([\"Loss\",\"Validation Loss\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36781e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_vgg_loss, trimmed_vgg_accuracy = trimmed_vgg.evaluate(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c32641",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Saving the final version\n",
    "trimmed_vgg.save('deepfake_audio_detector.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
