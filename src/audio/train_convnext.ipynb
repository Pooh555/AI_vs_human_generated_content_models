{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aadee827",
   "metadata": {},
   "source": [
    "# Constructing The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eae4d5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b726fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import IPython.display as ipd\n",
    "import librosa # Compatible with python 3.10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional  as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae8e4f",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e563732f",
   "metadata": {},
   "source": [
    "#### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign paths\n",
    "base_dir = \"dataset\"\n",
    "\n",
    "# Use for-norm dataset\n",
    "train_dir = os.path.join(base_dir, \"for-norm\", \"for-norm\", \"training\")\n",
    "test_dir = os.path.join(base_dir, \"for-norm\", \"for-norm\", \"testing\")\n",
    "val_dir = os.path.join(base_dir, \"for-norm\", \"for-norm\", \"validation\")\n",
    "\n",
    "# Classified directories\n",
    "train_dir_fake = os.path.join(train_dir, \"fake\")\n",
    "train_dir_real = os.path.join(train_dir, \"real\")\n",
    "test_dir_fake = os.path.join(test_dir, \"fake\")\n",
    "test_dir_real = os.path.join(test_dir, \"real\")\n",
    "val_dir_fake = os.path.join(val_dir, \"fake\")\n",
    "val_dir_real = os.path.join(val_dir, \"real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a108b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for mistakes\n",
    "print(\"Base directory:\", base_dir)\n",
    "print()\n",
    "print(\"Train directory:\", train_dir)\n",
    "print(\"Test directory:\", test_dir)\n",
    "print(\"Val directory:\", val_dir)\n",
    "print()\n",
    "print(\"Train (fake) directory:\", train_dir_fake)\n",
    "print(\"Train (real) directory:\", train_dir_real)\n",
    "print(\"Test (fake) directory:\", test_dir_fake)\n",
    "print(\"Test (real) directory:\", test_dir_real)\n",
    "print(\"Val (fake) directory:\", val_dir_fake)\n",
    "print(\"Val (real) directory:\", val_dir_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933b871e",
   "metadata": {},
   "source": [
    "#### Other Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d1e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch seed (for reproducibility purpose)\n",
    "SEED = 33\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCH = 20\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_WORKERS = 12\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0242a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # Free CUDA cache memory\n",
    "torch.manual_seed(SEED) # Assign Pytorch seed for reproducibility\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Initiate a CUDA device instance, if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d614eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current model\n",
    "def get_current_model_path(new_model=False):\n",
    "    count = 1\n",
    "\n",
    "    while (True):\n",
    "        if (os.path.isfile(\"../../trained_models/audio_convnext_model_\" + f\"{count}\" + \".pth\") == False):\n",
    "            if (new_model):\n",
    "                return \"../../trained_models/audio_convnext_model_\" + f\"{count}\" + \".pth\"\n",
    "            else:\n",
    "                return \"../../trained_models/audio_convnext_model_\" + f\"{count - 1}\" + \".pth\"\n",
    "        else:\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be882e7",
   "metadata": {},
   "source": [
    "## Obtain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc98ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain waveform (.wav) audio files\n",
    "train_fake_audio_path = [os.path.join(train_dir_fake, file) for file in os.listdir(train_dir_fake) if file.endswith(\".wav\")]\n",
    "train_real_audio_path = [os.path.join(train_dir_real, file) for file in os.listdir(train_dir_real) if file.endswith(\".wav\")]\n",
    "\n",
    "test_fake_audio_path = [os.path.join(test_dir_fake, file) for file in os.listdir(test_dir_fake) if file.endswith(\".wav\")]\n",
    "test_real_audio_path = [os.path.join(test_dir_real, file) for file in os.listdir(test_dir_real) if file.endswith(\".wav\")]\n",
    "\n",
    "validation_fake_audio_path = [os.path.join(val_dir_fake, file) for file in os.listdir(val_dir_fake) if file.endswith(\".wav\")]\n",
    "validation_real_audio_path = [os.path.join(val_dir_real, file) for file in os.listdir(val_dir_real) if file.endswith(\".wav\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e39741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training labels\n",
    "train_labels = []\n",
    "\n",
    "for i in train_fake_audio_path:\n",
    "    train_labels.append(0)\n",
    "for i in train_real_audio_path:\n",
    "    train_labels.append(1)\n",
    "\n",
    "# Convert list to Pandas dataframe\n",
    "train_labels_df = pd.DataFrame({'label':train_labels})\n",
    "\n",
    "# Get testing labels\n",
    "val_labels = []\n",
    "\n",
    "for i in validation_fake_audio_path:\n",
    "    val_labels.append(0)\n",
    "for i in validation_real_audio_path:\n",
    "    val_labels.append(1)\n",
    "\n",
    "# Convert list to Pandas dataframe\n",
    "val_labels_df = pd.DataFrame({'label':val_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709c1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5453ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641eae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Evaluate the training labels\n",
    "count = 0\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "    if (train_labels[i] == 0):\n",
    "        count = count+1\n",
    "\n",
    "print(train_labels)\n",
    "print(\"Number of total labels:\", len(train_labels))\n",
    "print(\"Number of fake audio samples:\", count)\n",
    "print(\"Number of real audio samples:\", len(train_labels) - count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ac4ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Evaluate the testing labels\n",
    "count = 0\n",
    "\n",
    "for i in range(len(val_labels)):\n",
    "    if (val_labels[i] == 0):\n",
    "        count = count+1\n",
    "\n",
    "print(val_labels)\n",
    "print(\"Number of total labels:\", len(val_labels))\n",
    "print(\"Number of fake audio samples:\", count)\n",
    "print(\"Number of real audio samples:\", len(val_labels) - count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0db9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random real training sample audio\n",
    "random_real_training_audio_file = train_real_audio_path[random.randint(0, 333)]\n",
    "ipd.Audio(random_real_training_audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29456f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random fake training sample audio\n",
    "random_fake_training_audio_file = train_fake_audio_path[random.randint(0, 333)]\n",
    "ipd.Audio(random_fake_training_audio_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0957f3b6",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da1b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "class CustomTrainingAudioDataset(Dataset):\n",
    "    def __init__(self, real_audio_files, fake_audio_files, target_shape):\n",
    "        self.real_files = real_audio_files\n",
    "        self.fake_files = fake_audio_files\n",
    "        self.target_shape = target_shape\n",
    "        self.all_files = self.real_files + self.fake_files\n",
    "        self.labels = [0] * len(self.fake_files) + [1] * len(self.real_files) # 0: fake, 1: real\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.all_files[idx]\n",
    "        label = self.labels[idx]\n",
    "        spectrogram = self._create_mel_spectrogram(file_path)\n",
    "        resized_spectrogram = cv2.resize(spectrogram, self.target_shape[::-1], interpolation=cv2.INTER_LINEAR)\n",
    "        # Convert to PyTorch tensor and add a channel dimension\n",
    "        resized_spectrogram = torch.tensor(resized_spectrogram).unsqueeze(0).float()\n",
    "        label = torch.tensor(label).long()\n",
    "        return resized_spectrogram, label\n",
    "\n",
    "    def _create_mel_spectrogram(self, file_path):\n",
    "        audio_data, sample_rate = librosa.load(file_path)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
    "        mel_decibel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        return mel_decibel_spectrogram\n",
    "\n",
    "# Testing\n",
    "class CustomTestingAudioDataset(Dataset):\n",
    "    def __init__(self, real_audio_files, fake_audio_files, target_shape):\n",
    "        self.real_files = real_audio_files\n",
    "        self.fake_files = fake_audio_files\n",
    "        self.target_shape = target_shape\n",
    "        self.all_files = self.real_files + self.fake_files\n",
    "        self.labels = [0] * len(self.fake_files) + [1] * len(self.real_files) # 0: fake, 1: real\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.all_files[idx]\n",
    "        label = self.labels[idx]\n",
    "        spectrogram = self._create_mel_spectrogram(file_path)\n",
    "        resized_spectrogram = cv2.resize(spectrogram, self.target_shape[::-1], interpolation=cv2.INTER_LINEAR)\n",
    "        # Convert to PyTorch tensor and add a channel dimension\n",
    "        resized_spectrogram = torch.tensor(resized_spectrogram).unsqueeze(0).float()\n",
    "        label = torch.tensor(label).long()\n",
    "        return resized_spectrogram, label\n",
    "\n",
    "    def _create_mel_spectrogram(self, file_path):\n",
    "        audio_data, sample_rate = librosa.load(file_path)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
    "        mel_decibel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        return mel_decibel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5649d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your target shape\n",
    "target_shape = (128, 87)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomTrainingAudioDataset(train_real_audio_path, train_fake_audio_path, target_shape)\n",
    "val_dataset = CustomTrainingAudioDataset(validation_real_audio_path, validation_fake_audio_path, target_shape)\n",
    "test_dataset = CustomTestingAudioDataset(test_real_audio_path, test_fake_audio_path, target_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d3e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display datasets' information\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3079f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7179bc",
   "metadata": {},
   "source": [
    "## Designing The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bd2d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained ConvNeXt Base model\n",
    "model = models.convnext_base(weights=\"DEFAULT\")\n",
    "\n",
    "# Freeze the backbone\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the first convolutional layer to accept 1 input channel\n",
    "first_conv_layer = model.features[0][0]\n",
    "in_channels = first_conv_layer.in_channels\n",
    "out_channels = first_conv_layer.out_channels\n",
    "kernel_size = first_conv_layer.kernel_size\n",
    "stride = first_conv_layer.stride\n",
    "padding = first_conv_layer.padding\n",
    "bias = first_conv_layer.bias is not None\n",
    "\n",
    "new_first_conv_layer = nn.Conv2d(1, out_channels, kernel_size, stride, padding, bias=bias)\n",
    "model.features[0][0] = new_first_conv_layer\n",
    "\n",
    "# Replace the classifier head with a custom one\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.AvgPool2d(kernel_size=3, padding=1, stride=3),  \n",
    "    nn.Flatten(),                 \n",
    "    nn.Linear(1024, 2),              \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2eb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to gpu\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5759fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function, optimizer, and learning rate scheduler\n",
    "loss_function = nn.CrossEntropyLoss() # Implement crossentropy loss\n",
    "optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=LEARNING_RATE) # AdamW\n",
    "# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4) # Initialize an optimizer\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f848ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping monitor\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1611fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_function, optimizer, val_loader=None, early_stopping=None, scheduler=None):\n",
    "    model.train()\n",
    "    \n",
    "    all_predictions = [] # all predictions from the training model after each epoch\n",
    "    all_labels = [] # all predictions from the training model after each epoch\n",
    "    train_losses = [] # Array of training losses in each epoch\n",
    "    lowest_val_loss = 1 # The lowest validation loss after an epoch\n",
    "    val_losses = [] # Array of validation losses for plotting graph\n",
    "    val_accuracies = [] # Array of validation accuracies for plotting graph\n",
    "\n",
    "    current_model_path = get_current_model_path(new_model=True)\n",
    "\n",
    "    progress_bar_length = 64 # Total number of \"#\" in the progress bar\n",
    "    \n",
    "    for epoch in range(EPOCH):\n",
    "        progress_accumulated = 0.0 # Fractional progress tracker\n",
    "        total_hashes = 0 # Total \"#\" printed so far\n",
    "\n",
    "        print(f\"Training epoch: {epoch + 1}\")\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, data in enumerate(dataloader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Collecting labels and predictions for classification report\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_accumulated, total_hashes = progress_bar(\n",
    "                dataloader, progress_accumulated, total_hashes, progress_bar_length\n",
    "            )\n",
    "\n",
    "        # After epoch completes, ensure the progress bar shows 100%\n",
    "        if total_hashes < progress_bar_length:\n",
    "            bar = \"#\" * progress_bar_length\n",
    "            print(f\"Completion (100.00%) : [{bar}]\", flush=True)\n",
    "        else:\n",
    "            print()  # Move to new line\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        # Save epoch loss and accuracy\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        print(f\"\\nTraining Loss: {epoch_loss:.4f}, Training Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        if val_loader:\n",
    "            val_loss, val_accuracy = validate_model(val_loader, model, loss_function)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "            print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "            save_best_model(lowest_val_loss, val_loss, current_model_path)\n",
    "\n",
    "            if (val_loss <= lowest_val_loss):\n",
    "                lowest_val_loss = val_loss\n",
    "\n",
    "            if early_stopping:\n",
    "                early_stopping(val_loss)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "                \n",
    "        # Call the scheduler to update the learning rate after each epoch\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(\"\\n\", end=\"\\r\")\n",
    "\n",
    "    if val_loader:\n",
    "        return train_losses, all_predictions, all_labels, val_losses, val_accuracies\n",
    "    else:\n",
    "        return train_losses, all_predictions, all_labels\n",
    "\n",
    "def validate_model(dataloader, model, loss_function):\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0  \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return val_loss / len(dataloader), accuracy\n",
    "\n",
    "def progress_bar(dataloader, accumulated, total_hashes, progress_bar_length):\n",
    "    total_batches = len(dataloader)\n",
    "    if total_batches == 0:\n",
    "        return accumulated, total_hashes\n",
    "    \n",
    "    # Calculate progress increment per batch\n",
    "    progress_per_batch = progress_bar_length / total_batches\n",
    "    accumulated += progress_per_batch # Add progress for current batch\n",
    "    \n",
    "    num_prints = int(accumulated) # Number of \"#\" to add now\n",
    "    if num_prints > 0:\n",
    "        total_hashes += num_prints\n",
    "        accumulated -= num_prints # Reset accumulated\n",
    "        \n",
    "        # Cap at 100% to avoid overshooting\n",
    "        if total_hashes > progress_bar_length:\n",
    "            total_hashes = progress_bar_length\n",
    "        \n",
    "        # Calculate percentage and print\n",
    "        percentage = (total_hashes / progress_bar_length) * 100\n",
    "        bar = \"=\" * (total_hashes - 1)\n",
    "\n",
    "        print(f\"Completion ({percentage:.2f}%) : [{bar}>]\", end=\"\\r\", flush=True)\n",
    "    \n",
    "    return accumulated, total_hashes\n",
    "\n",
    "# If the file exists already, change the number behind the file\n",
    "def save_best_model(lowest_val_loss, current_val_loss, current_model_path):\n",
    "    if (current_val_loss < lowest_val_loss):\n",
    "        torch.save(model.state_dict(), current_model_path) # save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec04e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # Free CUDA cache memory\n",
    "\n",
    "if val_dataloader:\n",
    "    epoch_losses, all_preds, all_labels, val_losses, val_accuracies = train_loop(\n",
    "        dataloader=train_dataloader,\n",
    "        model=model,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        val_loader=val_dataloader,\n",
    "        early_stopping=EarlyStopping(patience=PATIENCE)\n",
    "    )\n",
    "else:   \n",
    "    epoch_losses, all_preds, all_labels = train_loop(\n",
    "        dataloader=train_dataloader,\n",
    "        model=model,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        early_stopping=EarlyStopping(patience=PATIENCE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812d334",
   "metadata": {},
   "source": [
    "# Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcc61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epoch_losses, label=\"Training Loss\") # Training loss\n",
    "plt.plot(val_losses, label=\"Validation Loss\") # Validation loss\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()    \n",
    "plt.legend()    \n",
    "plt.grid(True)\n",
    "\n",
    "count = 1\n",
    "\n",
    "while (True):\n",
    "    if (os.path.isfile(\"../../trained_models/audio_convnext_model_\" + f\"{count}\" + \".pth\") == False):\n",
    "        plt.savefig(\"../../src/audio/graphs/audio_convnext_model_\" + f\"{count - 1}\" + \"_training_loss.png\") # Save the graph\n",
    "        break\n",
    "    else:\n",
    "        count += 1\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Accuracy graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(val_accuracies, label=\"Validation Accuracy\") # Validation accuracy\n",
    "plt.title(\"Validation accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "count = 1\n",
    "\n",
    "while (True):\n",
    "    if (os.path.isfile(\"../../trained_models/audio_convnext_model_\" + f\"{count}\" + \".pth\") == False):\n",
    "        plt.savefig(\"../../src/audio/graphs/audio_convnext_model_\" + f\"{count - 1}\" + \"_validation_accuracy.png\") # Save the graph\n",
    "        break\n",
    "    else:\n",
    "        count += 1\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Calculate the accuracy for the entire training set\n",
    "train_accuracy = 100 * sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560ebbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model_path = get_current_model_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54185b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained ConvNeXt Base model\n",
    "model = models.convnext_base(weights=\"DEFAULT\")\n",
    "\n",
    "# Freeze the backbone\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the first convolutional layer to accept 1 input channel\n",
    "first_conv_layer = model.features[0][0]\n",
    "in_channels = first_conv_layer.in_channels\n",
    "out_channels = first_conv_layer.out_channels\n",
    "kernel_size = first_conv_layer.kernel_size\n",
    "stride = first_conv_layer.stride\n",
    "padding = first_conv_layer.padding\n",
    "bias = first_conv_layer.bias is not None\n",
    "\n",
    "new_first_conv_layer = nn.Conv2d(1, out_channels, kernel_size, stride, padding, bias=bias)\n",
    "model.features[0][0] = new_first_conv_layer\n",
    "\n",
    "# Replace the classifier head with a custom one\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.AvgPool2d(kernel_size=3, padding=1, stride=3),  \n",
    "    nn.Flatten(),                 \n",
    "    nn.Linear(1024, 2),              \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf70109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model.load_state_dict(torch.load(current_model_path)) # load the trained model\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b303ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Initialize lists to store predictions and true labels\n",
    "all_test_preds = []\n",
    "all_test_labels = []\n",
    "\n",
    "# Disable gradient calculations for evaluation\n",
    "with torch.no_grad():\n",
    "    # Iterate over the test dataloader\n",
    "    for data in test_dataloader:\n",
    "        # Get inputs and labels, and move them to the correct device\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get the predicted class (the one with the highest probability)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Append the predicted classes and true labels to the lists\n",
    "        # Move predictions and labels to CPU and convert to numpy arrays for scikit-learn\n",
    "        all_test_preds.extend(predicted.cpu().numpy())\n",
    "        all_test_labels.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a9d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class names based on the label encoding used in the dataset (0: fake, 1: real)\n",
    "target_names = ['fake', 'real']\n",
    "\n",
    "# Generate and print the classification report\n",
    "print(\"Classification Report on Test Data:\")\n",
    "report = classification_report(all_test_labels, all_test_preds, target_names=target_names)\n",
    "print(report)\n",
    "\n",
    "# Generate and print the confusion matrix\n",
    "print(\"\\nConfusion Matrix on Test Data:\")\n",
    "cm = confusion_matrix(all_test_labels, all_test_preds)\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
